{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unit 6 - Automatic differentiation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Ykx2mpxb37"
      },
      "source": [
        "# Fix Ransom Seed\n",
        "\n",
        "A random seed (or seed state, or just seed) is a number (or vector) used to **initialize** a pseudorandom number generator\\\n",
        "So if the execution sequence after setting the seed is the same, the random numbers sequence will be the same.\\\n",
        "Each run of the cell will generate the new random number (a new step in the execution sequence), even though its the same cell.\\\n",
        "So if you want the cell to generate the exact same number, you need to put the seed fixing code into that cell. So that each run of that cell will initialize the seed to the same value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzeuOmpuwyK7"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import random\n",
        "random.seed(0)\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eda_5xwlmjT"
      },
      "source": [
        "# Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFdi8Q52lTXu"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqO3toYFvD2Z"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wogh9a3YvGOq",
        "outputId": "3457c464-fbce-417d-850d-f02c84bd6a80"
      },
      "source": [
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)  # torch.tensor has requires_grad=False\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w) + b  # matmul(): 1-dim x is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed. {math: (x**T * w)**T}\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "print(loss)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.3513, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcOKE5Is_LXm",
        "outputId": "d5e8824a-c12e-425d-8634-43ca3e767805"
      },
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7f14ea0b1790>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f14ea0b17d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2fSZZJsD2dQ",
        "outputId": "a1f97a75-7fc2-4a68-aa33-fafdaab8dde1"
      },
      "source": [
        "# --- If run backward() second time. Set all the gradient of leaf tensor to 0 firt, or else grad will be accumulated --- #\n",
        "if w.grad != None and b.grad != None:                                                                                    #\n",
        "    w.grad.zero_()                                                                                                       #\n",
        "    b.grad.zero_()                                                                                                       # \n",
        "# ---------- No need for above tow line, if only backwar() once. Because no grad before calling backward() yet --------- #\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628]])\n",
            "tensor([0.2444, 0.2308, 0.2628])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUy_mS0hEIAF",
        "outputId": "448c3e1e-598a-452f-cecc-9fbc200a93de"
      },
      "source": [
        "print(z.requires_grad)\n",
        "print(z.grad)  # None: Although z.requires_grad=True, it is not a leaf Tensor. So its .grad attribute won't be populated during autograd.backward()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKG0QoOKKEnn",
        "outputId": "0aff11f7-25ff-4aee-e617-bb9ad725531c"
      },
      "source": [
        "# New calculation (from leaf tensor to backwarded tensor), build new graph, can call backward again no matter how\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "print(loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.3513, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic0sFvMgGCbO",
        "outputId": "85c4128e-83ab-497d-9226-8d5d7125b522"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "loss.backward(retain_graph=True)  # Specify retain_graph=True, then backward() on same graph (no new claculation) can be called one more time\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628]])\n",
            "tensor([0.2444, 0.2308, 0.2628])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5XDjhNFPp37",
        "outputId": "0cafb620-5691-4b50-d85b-6fac570bc090"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "loss.backward()  # Default retain_graph=None, then backward() on same graph (no new claculation) cannot be called\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628],\n",
            "        [0.2444, 0.2308, 0.2628]])\n",
            "tensor([0.2444, 0.2308, 0.2628])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "jS8-8TY-P4iK",
        "outputId": "e85af129-7584-412b-cb91-65e86b880ad8"
      },
      "source": [
        "loss.backward()  # RuntimeError: Trying to backward through the graph a second time"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2eecda104dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# RuntimeError: Trying to backward through the graph a second time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVmtEKHsU1Mb",
        "outputId": "a0d66fac-31f5-4f70-f40b-d5ef78a47651"
      },
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "print(w.requires_grad, b.requires_grad)  # tensor not calculated by in no_grad() block, won't be affected"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8r2rilXVQNE",
        "outputId": "cae103a1-42c5-41f9-f06c-1c956beffabc"
      },
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "z_det = z.detach()  # detach(): Returns a new Tensor, detached from the current graph. The result will never require gradient.\n",
        "print(z_det.requires_grad)\n",
        "print(z.requires_grad)  # Not affected"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGUDsIObWNcy",
        "outputId": "cb41d053-3ac7-454d-f6a5-e896290f02b7"
      },
      "source": [
        "inp = torch.eye(5, requires_grad=True)  # same as inp=torch.flatten(torch.eye(5)).requires_grad_(), but result being reshaped to the size\n",
        "out = (inp + 1).pow(2)\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"First call\\n {inp.grad}\")\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n {inp.grad}\")\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nCall after zeroing gradients\\n {inp.grad}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First call\n",
            " tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.],\n",
            "        [2., 2., 2., 2., 4.]])\n",
            "\n",
            "Second call\n",
            " tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.],\n",
            "        [4., 4., 4., 4., 8.]])\n",
            "\n",
            "Call after zeroing gradients\n",
            " tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.],\n",
            "        [2., 2., 2., 2., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyYCyFXhkodm",
        "outputId": "c8a79e78-eb31-48a2-e200-e746c1ea1506"
      },
      "source": [
        "# For details,see https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd\n",
        "inp = torch.tensor([1.,0.,0.], requires_grad=True)\n",
        "print(inp)\n",
        "print(f\"\\nJ = \\n{torch.tensor([[4, 0, 0], [0, 2, 0], [0, 0, 2]])}\\n\")\n",
        "out = (inp + 1).pow(2)\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"First call\\n {inp.grad}\")\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n {inp.grad}\")\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nCall after zeroing gradients\\n {inp.grad}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 0., 0.], requires_grad=True)\n",
            "\n",
            "J = \n",
            "tensor([[4, 0, 0],\n",
            "        [0, 2, 0],\n",
            "        [0, 0, 2]])\n",
            "\n",
            "First call\n",
            " tensor([4., 2., 2.])\n",
            "\n",
            "Second call\n",
            " tensor([8., 4., 4.])\n",
            "\n",
            "Call after zeroing gradients\n",
            " tensor([4., 2., 2.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}